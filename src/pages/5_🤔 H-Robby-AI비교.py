import streamlit as st
import os
import openai
from langchain_community.chat_models import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from transformers import pipeline, GPT2Config, GPT2LMHeadModel, GPT2Tokenizer
from modules.chatbot2 import Chatbot2
# import llm_blender
# from transformers import AutoTokenizer, AutoModelForCausalLM

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
from dotenv import load_dotenv
load_dotenv()

# OpenAI API í‚¤ ë¡œë“œ
openai.api_key = os.getenv('OPENAI_API_KEY')

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(layout="wide", page_icon="ğŸ¤–", page_title="AI ëª¨ë¸ ë¹„êµ")

# Chatbot ê°ì²´ ìƒì„±
# chatbot = Chatbot()

# ì‚¬ìš©ì ì •ì˜ ê°€ì¤‘ì¹˜ ìŠ¬ë¼ì´ë” (ì•™ìƒë¸”ìš©)
# st.sidebar.title("LLM ì•™ìƒë¸” ì„¤ì •")
# openai_weight = st.sidebar.slider("OpenAI ê°€ì¤‘ì¹˜", 0.0, 1.0, 0.33)
# gemini_weight = st.sidebar.slider("Gemini ê°€ì¤‘ì¹˜", 0.0, 1.0, 0.33)
# huggingface_weight = st.sidebar.slider("HuggingFace ê°€ì¤‘ì¹˜", 0.0, 1.0, 0.34)


# # ëª¨ë¸ ì´ˆê¸°í™”
# openai_model = ChatOpenAI(model="gpt-4", temperature=0.7)
# gemini_model = ChatOpenAI(model="gpt-4o-mini", temperature=0.7)  # Gemini ëª¨ë¸ ì˜ˆì‹œ
# # huggingface_model = pipeline(model="bigscience/bloom", temperature=0.7)
# config = GPT2Config.from_pretrained("skt/kogpt2-base-v2")
# huggingface_tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
# huggingface_model = GPT2LMHeadModel.from_pretrained("gpt2")

# # í…œí”Œë¦¿ ì„¤ì •
# chat_prompt_template = ChatPromptTemplate.from_template(prompt_template)

# ì§ˆì˜ ì…ë ¥ UI
st.markdown(
    """
    <head>
        <link href="https://fonts.googleapis.com/css2?family=IBM+Plex+Sans+KR:wght@400;600&display=swap" rel="stylesheet">
    </head>
    <h1 style='text-align: center; font-family: "IBM Plex Sans KR", sans-serif;'> H-Robbyê°€ AI ëª¨ë¸ì„ ë¹„êµí•´ë“œë ¤ìš” ğŸ¤”</h1>
    """,
    unsafe_allow_html=True,
)

st.write("ì§ˆë¬¸ì„ ì…ë ¥í•˜ê³  ê° ëª¨ë¸ì˜ ì‘ë‹µ ë° ë¹„ìš©ì„ ë¹„êµí•´ë³´ì„¸ìš”.")
user_query = st.text_input("ì§ˆë¬¸ ì…ë ¥", placeholder="ì˜ˆ: í˜„ëŒ€í“¨ì²˜ë„·ì€ ì–´ë””ì— ìœ„ì¹˜í•´ìˆë‚˜ìš”?")

# Chatbot2 ê°ì²´ ì´ˆê¸°í™”
retriever = None  # Retrieverë¥¼ í•„ìš”ì— ë”°ë¼ ì´ˆê¸°í™”
openai_chatbot = Chatbot2("OpenAI", "gpt-4", 0.7, retriever)
gemini_chatbot = Chatbot2("Gemini", "gemini-pro", 0.7, retriever)
huggingface_chatbot = Chatbot2("HuggingFace", "gpt2", 0.7, retriever)

# ëª¨ë¸ ì‘ë‹µ ì»¨í…Œì´ë„ˆ
if user_query:
    with st.spinner("ëª¨ë¸ ì‘ë‹µì„ ìƒì„± ì¤‘..."):
        # OpenAI ì‘ë‹µ
        openai_response = openai_chatbot.chat(user_query)
        openai_cost = len(user_query) * 0.02  # ì˜ˆì‹œ ë¹„ìš© ê³„ì‚°

        # Gemini ì‘ë‹µ
        gemini_response = gemini_chatbot.chat(user_query)
        gemini_cost = len(user_query) * 0.015  # ì˜ˆì‹œ ë¹„ìš© ê³„ì‚°

        # HuggingFace ì‘ë‹µ
        huggingface_response = huggingface_chatbot.chat(user_query)
        huggingface_cost = len(user_query) * 0.01  # ì˜ˆì‹œ ë¹„ìš© ê³„ì‚°


        # # HuggingFace (GPT-2) ì‘ë‹µ
        # inputs = huggingface_tokenizer(user_query, return_tensors="pt")
        # outputs = huggingface_model.generate(inputs['input_ids'], max_length=100, num_return_sequences=1)
        # huggingface_response = huggingface_tokenizer.decode(outputs[0], skip_special_tokens=True)
        # huggingface_cost = len(user_query) * 0.01  # ì˜ˆì‹œ ê³„ì‚°

        # Load the Blender ranker and fuser
        # blender = llm_blender.Blender()
        # blender.loadranker("llm-blender/PairRM")
        # blender.loadfuser("llm-blender/gen_fuser_3b")

        # ì•™ìƒë¸” ì‘ë‹µ (ê°€ì¤‘ì¹˜ ê¸°ë°˜)
        # ensemble_response = (
        #     f"{openai_weight * openai_response} + "
        #     f"{gemini_weight * gemini_response} + "
        #     f"{huggingface_weight * huggingface_response}"
        # )

        # ê²°ê³¼ í‘œì‹œ
        st.markdown("## ëª¨ë¸ ì‘ë‹µ ë¹„êµ")
        col1, col2, col3 = st.columns(3)

        with col1:
            st.markdown("### OpenAI")
            st.text_area("OpenAIì‘ë‹µ", value=openai_response, height=350)
            # st.write(f"ë¹„ìš©: ${333:.4f}")
            st.write(f"ë¹„ìš©: ${openai_cost:.4f}")

        with col2:
            st.markdown("### Gemini")
            st.text_area("Geminiì‘ë‹µ", value=gemini_response, height=350)
            # st.write(f"ë¹„ìš©: ${222:.4f}")
            st.write(f"ë¹„ìš©: ${gemini_cost:.4f}")

        with col3:
            st.markdown("### HuggingFace")
            st.text_area("HuggingFaceì‘ë‹µ", value="huggingface_response", height=350)
            # st.write(f"ë¹„ìš©: ${111:.4f}")
            st.write(f"ë¹„ìš©: ${huggingface_cost:.4f}")

        # st.markdown("## ì•™ìƒë¸” ì‘ë‹µ")
        # responses = {openai_response, gemini_response, huggingface_response}

        # # Rank responses
        # ranked = blender.rank_with_ref(user_query, list(responses.values()), return_scores=True, batch_size=1)
        
        # # Select the top response
        # top_response = max(ranked[0], key=lambda x: x[1])
        # print(f"Top response: {top_response[0]}")

        # # Optionally fuse responses for a refined answer
        # fused_response = blender.fuse(user_query, list(responses.values()), batch_size=1)

        # st.text_area("ì•™ìƒë¸” ê²°ê³¼", value=fused_response, height=150)
        # # st.text_area("ì•™ìƒë¸” ê²°ê³¼", value="ensemble_response", height=150)
        # # st.text_area("ì•™ìƒë¸” ê²°ê³¼", value=ensemble_response, height=150)

def chat_with_user(user_message):
    ai_message = chain.invoke(user_message)
    return ai_message

# ì•™ìƒë¸” ì„¤ì • ê°€ì´ë“œ
# st.sidebar.markdown(
#     """
#     ### ğŸ“Œ ê°€ì´ë“œ
#     - ê° ëª¨ë¸ì˜ ì‘ë‹µ ê°€ì¤‘ì¹˜ë¥¼ ì¡°ì •í•´ ì•™ìƒë¸” ì‘ë‹µ ê²°ê³¼ë¥¼ ë³€ê²½í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
#     - ê°€ì¤‘ì¹˜ í•©ì€ **1.0**ì´ ë˜ì–´ì•¼ ê°€ì¥ ì •í™•í•œ ê²°ê³¼ë¥¼ ê¸°ëŒ€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
#     """
# )
